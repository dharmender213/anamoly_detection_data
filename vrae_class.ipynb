{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de35e38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in /home/astro-s6/anaconda3/lib/python3.7/site-packages (2.9.1)\n",
      "Requirement already satisfied: packaging in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (61.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.21.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.9.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/astro-s6/anaconda3/lib/python3.7/site-packages (from packaging->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: Keras in /home/astro-s6/anaconda3/lib/python3.7/site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu\n",
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34fb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d56876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 16:06:46.779358: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-23 16:06:46.779379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a78784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "print(tc.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23206f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c104a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from tensorflow.keras.layers import SimpleRNN,Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import KLDivergence,binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob,os\n",
    "from tensorflow.keras.layers import Dense,GRU, Flatten, Conv2D, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import (Callback, TensorBoard, EarlyStopping,\n",
    "                             ModelCheckpoint, CSVLogger, ProgbarLogger)\n",
    "from tensorflow.keras.layers import (Input, Dense, TimeDistributed, LSTM, GRU, Dropout,  \n",
    "                         Concatenate, Flatten, RepeatVector, Lambda, Bidirectional, SimpleRNN)#merge,\n",
    "import sys\n",
    "import csv\n",
    "#from collections import Iterable, OrderedDict\n",
    "import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b2a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from collections.abc import Iterable\n",
    "except ImportError:\n",
    "    from collections import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007496c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.6\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947bced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lcs_file ='lcs.npy'\n",
    "except ImportError:\n",
    "    print('please use npy file only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c109092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18382\n",
      "[[5.82022673e+04 1.92292023e+01 6.47380054e-02]\n",
      " [5.82022742e+04 1.91007309e+01 5.87802716e-02]\n",
      " [5.82072615e+04 1.92533073e+01 6.59256652e-02]\n",
      " [5.82102655e+04 1.93263912e+01 6.96673542e-02]\n",
      " [5.82102735e+04 1.91409512e+01 6.05796054e-02]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lcs_file ='lcs.npy'\n",
    "lcs =np.load(lcs_file,allow_pickle=True)\n",
    "print(len(lcs))\n",
    "print(lcs[10000][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ac3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.82022764e+04, 1.85910397e+01, 4.75157984e-02],\n",
       "       [5.82052526e+04, 1.86000938e+01, 4.78192680e-02],\n",
       "       [5.82052715e+04, 1.85581970e+01, 4.64340188e-02],\n",
       "       [5.82062694e+04, 1.85672989e+01, 4.67308350e-02],\n",
       "       [5.82082824e+04, 1.84853821e+01, 4.41396125e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#shuffling the data to avoid systematics \n",
    "np.random.seed(42)\n",
    "np.random.shuffle(lcs)\n",
    "lcs[100][0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9daa3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "lcs_raw = pad_sequences(lcs, value=np.nan, dtype='float', padding='post')\n",
    "#print(lcs_raw[100][0:5])\n",
    "#print(lcs_raw[:,:,1])\n",
    "#print(np.nanmax(lcs_raw[:, :, 1], axis=1)) this is magnitude value\n",
    "#print(np.inf)\n",
    "#print(np.all(np.isnan(lcs_raw[:, :, 1])) | (np.nanmax(lcs_raw[:, :, 1], axis=1) > np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ededbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions taken from https://github.com/yutarotachibana/CatalinaQSO_AutoEncoder\n",
    "\n",
    "def times_to_lags(T):\n",
    "    \"\"\"(N x n_step) matrix of times -> (N x n_step) matrix of lags.\n",
    "    First time is assumed to be zero.\n",
    "    \"\"\"\n",
    "    assert T.ndim == 2, \"T must be an (N x n_step) matrix\"\n",
    "    return np.c_[np.diff(T, axis=1)/365., np.zeros(T.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aab907d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_raw, m_max=np.inf):\n",
    "    X = X_raw.copy()\n",
    "    #print(X)\n",
    "    wrong_units =  np.all(np.isnan(X[:, :, 1])) | (np.nanmax(X[:, :, 1], axis=1) > m_max)\n",
    "    #print(wrong_units)\n",
    "    X = X[~wrong_units, :, :]\n",
    "    X[:, :, 0] = times_to_lags(X[:, :, 0])\n",
    "    #print(X[:, :, 0])\n",
    "    means = np.atleast_2d(np.nanmean(X[:, :, 1], axis=1)).T\n",
    "    #print(means)\n",
    "    X[:, :, 1] -= means\n",
    "    scales = np.atleast_2d(np.nanstd(X[:, :, 1], axis=1)).T\n",
    "    #print(scales)\n",
    "    X[:, :, 1] /= scales\n",
    "    errors = X[:, :, 2] / scales\n",
    "    #print(errors)\n",
    "    X = X[:, :, :2]\n",
    "    #print(X) is lcs_scaled value\n",
    "    return X, means, scales, errors, wrong_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f9a1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs_scaled, means, scales, errors, wrong_units = preprocess(lcs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020a08d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.15391918e-03 1.44299832e+00]\n",
      " [5.18772603e-05 1.53424018e+00]\n",
      " [2.73398658e-03 1.11202903e+00]\n",
      " [5.51509397e-03 1.20375259e+00]\n",
      " [1.91030567e-02 3.78243564e-01]]\n",
      "[18.44784829]\n"
     ]
    }
   ],
   "source": [
    "print(lcs_scaled[100][0:5])\n",
    "print(means[100])\n",
    "#print(lcs_scaled[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d70f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(lcs_scaled.shape[1], 2), name='main_input') #(lag, mag)\n",
    "aux_input = Input(shape=(lcs_scaled.shape[1], 1), name='aux_input') #(lag)\n",
    "#print(aux_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "516f5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = [main_input, aux_input]\n",
    "\n",
    "sample_weight = 1. / errors\n",
    "#print(sample_weight)\n",
    "sample_weight[np.isnan(sample_weight)] = 0.0\n",
    "lcs_scaled[np.isnan(lcs_scaled)] = 0.\n",
    "\n",
    "#lcs_scaled[:, :, [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc186025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astro-s6/anaconda3/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Initialization \n",
    "lr = 1e-3 #learning rate \n",
    "optimizer = Adam(lr=lr)\n",
    "#output_size=16\n",
    "output_size =2\n",
    "#gru_size = 32\n",
    "gru_size =10\n",
    "#nepochs = 2000\n",
    "nepochs =1000\n",
    "#batchsize = 512\n",
    "batchsize =100\n",
    "#dropout_val = 0.25\n",
    "dropout_val = 0.20\n",
    "\n",
    "resume_training = False # if True use W&B to recover weights and resume training, if False train a new model\n",
    "\n",
    "def sampling(samp_args):\n",
    "    z_mean, z_log_sigma = samp_args\n",
    "\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_sigma) * epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea9c1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 16:07:17.829954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-23 16:07:17.829974: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-23 16:07:17.829988: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dharmender): /proc/driver/nvidia/version does not exist\n",
      "2022-08-23 16:07:17.830534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " main_input (InputLayer)        [(None, 1850, 2)]    0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 1850, 20)     840         ['main_input[0][0]']             \n",
      "                                                                                                  \n",
      " drop_encoder1 (Dropout)        (None, 1850, 20)     0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 20)          1920        ['drop_encoder1[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " drop_encoder2 (Dropout)        (None, 20)           0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " encoding_mean (Dense)          (None, 2)            42          ['drop_encoder2[0][0]']          \n",
      "                                                                                                  \n",
      " encoding_log_var (Dense)       (None, 2)            42          ['drop_encoder2[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 2)            0           ['encoding_mean[0][0]',          \n",
      "                                                                  'encoding_log_var[0][0]']       \n",
      "                                                                                                  \n",
      " aux_input (InputLayer)         [(None, 1850, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " repeat (RepeatVector)          (None, 1850, 2)      0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1850, 3)      0           ['aux_input[0][0]',              \n",
      "                                                                  'repeat[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder1 (GRU)                 (None, 1850, 10)     450         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " drop_decoder1 (Dropout)        (None, 1850, 10)     0           ['decoder1[0][0]']               \n",
      "                                                                                                  \n",
      " decoder2 (GRU)                 (None, 1850, 10)     660         ['drop_decoder1[0][0]']          \n",
      "                                                                                                  \n",
      " time_dist (TimeDistributed)    (None, 1850, 1)      11          ['decoder2[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 2)           0           ['encoding_log_var[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.exp (TFOpLambda)       (None, 2)            0           ['encoding_log_var[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 2)            0           ['tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.math.exp[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.square (TFOpLambda)    (None, 2)            0           ['encoding_mean[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 2)           0           ['tf.math.subtract[0][0]',       \n",
      " )                                                                'tf.math.square[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None,)             0           ['tf.math.subtract_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None,)              0           ['tf.math.reduce_sum[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  ()                  0           ['tf.math.multiply[0][0]']       \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   ()                   0           ['tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " add_loss (AddLoss)             ()                   0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,965\n",
      "Trainable params: 3,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "encoder = Bidirectional(GRU(gru_size, name='encoder1', return_sequences=True))(main_input)\n",
    "encoder = Dropout(dropout_val, name='drop_encoder1')(encoder) \n",
    "encoder = Bidirectional(GRU(gru_size, name='encoder2', return_sequences=False))(encoder)\n",
    "encoder = Dropout(dropout_val, name='drop_encoder2')(encoder)\n",
    "codings_mean = Dense(units=output_size, name='encoding_mean', activation='linear')(encoder)\n",
    "codings_log_var = Dense(units=output_size, name='encoding_log_var', activation='linear')(encoder) \n",
    "codings = Lambda(sampling, output_shape=(output_size,))([codings_mean, codings_log_var])\n",
    "\n",
    "\n",
    "#decoder\n",
    "decoder = RepeatVector(lcs_scaled.shape[1], name='repeat')(codings)\n",
    "#decoder = tf.keras.layers.merge.concatenate([aux_input, decoder])\n",
    "decoder = tf.keras.layers.concatenate([aux_input, decoder])\n",
    "decoder = GRU(gru_size, name='decoder1', return_sequences=True)(decoder)\n",
    "decoder = Dropout(dropout_val, name='drop_decoder1')(decoder)\n",
    "decoder = GRU(gru_size, name='decoder2', return_sequences=True)(decoder)\n",
    "decoder = TimeDistributed(Dense(1, activation='linear'), name='time_dist')(decoder)\n",
    "\n",
    "#VAE\n",
    "model = Model(model_input, decoder)\n",
    "\n",
    "latent_loss = -0.5*K.sum(1+codings_log_var-K.exp(codings_log_var)-K.square(codings_mean),axis=-1)\n",
    "model.add_loss(K.mean(latent_loss)/200.)\n",
    "model.compile(optimizer=optimizer, loss='mse',  metrics=[tf.keras.metrics.MeanAbsoluteError()], weighted_metrics=[tf.keras.metrics.MeanAbsoluteError()], sample_weight_mode='temporal')#,run_eagerly=True)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0aa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './lcs_image/'\n",
    "weights_path = os.path.join(log_dir, 'weights_lr1e3_4paper.h5')\n",
    "logs = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1626c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.EarlyStopping object at 0x7fd98fb182d0>\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, mode='auto', restore_best_weights=True)\n",
    "print(early_stopping)\n",
    "check_points = tf.keras.callbacks.ModelCheckpoint(filepath=weights_path, save_freq='epoch', save_weights_only=False, save_best_only=True, monitor='val_loss', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9556e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 16:07:29.664625: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 217634000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "148/148 [==============================] - ETA: 0s - loss: 0.4083 - mean_absolute_error: 0.2884 - weighted_mean_absolute_error: 0.7397\n",
      "Epoch 1: val_loss improved from inf to 0.40822, saving model to ./lcs_image/weights_lr1e3_4paper.h5\n",
      "148/148 [==============================] - 269s 2s/step - loss: 0.4083 - mean_absolute_error: 0.2884 - weighted_mean_absolute_error: 0.7397 - val_loss: 0.4082 - val_mean_absolute_error: 0.2822 - val_weighted_mean_absolute_error: 0.7438\n",
      "Epoch 2/1000\n",
      " 20/148 [===>..........................] - ETA: 3:41 - loss: 0.3948 - mean_absolute_error: 0.2727 - weighted_mean_absolute_error: 0.7439"
     ]
    }
   ],
   "source": [
    "#aux input contains the delta times, Y (lcs_scaled[:, :, [1]]) is the arrey with the normalized magnitudes, sample_weights is 1/err\n",
    "history = model.fit({'main_input': lcs_scaled, 'aux_input': np.delete(lcs_scaled, 1, axis=2)},\n",
    "                    lcs_scaled[:, :, [1]], \n",
    "                    epochs=nepochs, \n",
    "                    batch_size=batchsize,\n",
    "                    sample_weight=sample_weight,\n",
    "                    callbacks = [\n",
    "                                check_points,\n",
    "                                early_stopping\n",
    "                                 ],\n",
    "                    validation_split=0.2,\n",
    "                    )\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fab3b1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270010/3655483651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the train loss is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the validation loss is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_mae\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "print('the train loss is',train_loss)\n",
    "val_loss   = history.history['val_loss']\n",
    "print('the validation loss is',val_loss)\n",
    "train_mae  = history.history['mean_absolute_error']\n",
    "val_mae    = history.history['val_mean_absolute_error']\n",
    "train_wmae  = history.history['weighted_mean_absolute_error']\n",
    "val_wmae    = history.history['val_weighted_mean_absolute_error']\n",
    "xc         = range(len(train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e91ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame({'epoch': xc, 'train_loss': train_loss, 'val_loss': val_loss, \n",
    "                           'train_mae': train_mae, 'val_mae': val_mae,\n",
    "                           'train_wmae': train_wmae, 'val_wmae': val_wmae})\n",
    "\n",
    "history_path = os.path.join(log_dir, 'train_history_4paper_lr1e3.csv')\n",
    "\n",
    "\n",
    "df_history.to_csv(history_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76373d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xc)\n",
    "#data=pd.read_csv(history_path)\n",
    "#print(data)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d742f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xc, train_loss)\n",
    "plt.plot(xc, val_loss)\n",
    "plt.savefig('./lcs_image/xc_val_loss_and_train_loss.png')\n",
    "\n",
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()\n",
    "plt.savefig('./lcs_image/GRU2x32-encoding16_loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_model = Model(model.input, model.get_layer('lambda').output)\n",
    "decode_model = Model(model.input, model.output)\n",
    "print(encode_model)\n",
    "print(decode_model)\n",
    "encoding = encode_model.predict({'main_input': lcs_scaled, 'aux_input': np.delete(lcs_scaled, 1, axis=2)})\n",
    "print(encoding)\n",
    "decoding = decode_model.predict({'main_input': lcs_scaled, 'aux_input': np.delete(lcs_scaled, 1, axis=2)})\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4adcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = encoding[0:int(0.8*len(lcs))]\n",
    "print(encoding_train)\n",
    "encoding_val = encoding[int(0.8*len(lcs)):len(lcs)]\n",
    "decoding_train = decoding[0:int(0.8*len(lcs))]\n",
    "decoding_val = decoding[int(0.8*len(lcs)):len(lcs)]\n",
    "X_raw_train = lcs_raw[0:int(0.8*len(lcs))]\n",
    "X_raw_val = lcs_raw[int(0.8*len(lcs)):len(lcs)]\n",
    "X_train = lcs_scaled[0:int(0.8*len(lcs))]\n",
    "X_val = lcs_scaled[int(0.8*len(lcs)):len(lcs)]\n",
    "scales_train = scales[0:int(0.8*len(lcs))]\n",
    "scales_val = scales[int(0.8*len(lcs)):len(lcs)]\n",
    "means_train = means[0:int(0.8*len(lcs))]\n",
    "means_val = means[int(0.8*len(lcs)):len(lcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6de16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 150 #init num of the plotting data\n",
    "#i=15000\n",
    "k=0 #initialize the counter\n",
    "fignum = 5\n",
    "plt.figure(figsize=(12, 4))\n",
    "#plotting raw-lightcurves and the decoded lightcurves \n",
    "for num in range(i,i+fignum):\n",
    "    plt.subplot(2, fignum, k+1)\n",
    "    #print(X_raw_train[num][:,0])\n",
    "    #print(365.0*np.cumsum(X_train[num][:, 0][X_train[num][:, 0]>0])+X_raw_train[num][0,0])\n",
    "    plt.errorbar(X_raw_train[num][:,0], X_raw_train[num][:,1], yerr=X_raw_train[num][:, 2], fmt='o', color='black', ms=5)\n",
    "    plt.errorbar(365.0*np.cumsum(X_train[num][:, 0][X_train[num][:, 0]>0])+X_raw_train[num][0,0], \n",
    "             decoding_train[num][X_train[num][:, 0]>0]*scales_train[num]+means_train[num], fmt='o', color='orange', ms=7)\n",
    "    plt.ylim(plt.ylim()[::-1])\n",
    "    k += 1\n",
    "#plotting encoded features\n",
    "k=0\n",
    "for num in range(i+fignum,i+2*fignum):\n",
    "    plt.subplot(2, fignum, k+1+fignum)\n",
    "    plt.imshow(encoding_train[num].reshape(4,4), vmin=-2, vmax=2, cmap='viridis_r')\n",
    "    k += 1\n",
    "plt.tight_layout()\n",
    "plt.savefig('./lcs_image/ZTF_alerts_modeled_lcs_training_set.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"Upper panels: Input light curves (black points) and decoded light curves (orange points) \\n\n",
    "Lower panels: Encoded 16 features\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 1650 #init num of the plotting data\n",
    "i=16\n",
    "k=0 #initialize the counter\n",
    "fignum = 5\n",
    "plt.figure(figsize=(12, 4))\n",
    "#plotting raw-lightcurves and the decoded lightcurves \n",
    "for num in range(i,i+fignum):\n",
    "    plt.subplot(2, fignum, k+1)\n",
    "    plt.errorbar(X_raw_val[num][:,0], X_raw_val[num][:,1], yerr=X_raw_val[num][:, 2], fmt='o', color='black', ms=5)\n",
    "    plt.errorbar(365.0*np.cumsum(X_val[num][:, 0][X_val[num][:, 0]>0])+X_raw_val[num][0,0], \n",
    "             decoding_val[num][X_val[num][:, 0]>0]*scales_val[num]+means_val[num], fmt='o', color='orange', ms=7)\n",
    "    plt.ylim(plt.ylim()[::-1])\n",
    "    k += 1\n",
    "#plotting encoded features\n",
    "k=0\n",
    "for num in range(i+fignum,i+2*fignum):\n",
    "    plt.subplot(2, fignum, k+1+fignum)\n",
    "    plt.imshow(encoding_val[num].reshape(4,4), vmin=-2, vmax=2, cmap='viridis_r')\n",
    "    k += 1\n",
    "plt.tight_layout()\n",
    "plt.savefig('./lcs_image/ZTF_alerts_modeled_lcs_validation_set.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_redchisq(x, x_pred, weight):\n",
    "    mask =  (~np.isnan(weight))\n",
    "    out = np.sum(((x[mask]-x_pred[mask])*weight[mask])**2)/len(weight[mask])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5379e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "RedChiSq = []\n",
    "print('Caluculating reduced chi-square for each source')\n",
    "for m in range(0, len(lcs_raw)):\n",
    "    if m%100 == 0:\n",
    "        print('.', end='')\n",
    "    RedChiSq.append(calc_redchisq(lcs_scaled[m][:,1]*scales[m]+means[m], np.squeeze(decoding[m]*scales[m]+means[m]), 1/lcs_raw[m][:,2]))\n",
    "    \n",
    "RedChiSq_train = RedChiSq[0:int(0.8*len(lcs))]\n",
    "RedChiSq_val = RedChiSq[int(0.8*len(lcs)):len(lcs)]    \n",
    "\n",
    "print(\"training chi2 median, mean: \",np.median(np.array(RedChiSq_train)),np.mean(np.array(RedChiSq_train)))\n",
    "print(\"validation chi2 median, mean: \",np.median(np.array(RedChiSq_val)),np.mean(np.array(RedChiSq_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.array(RedChiSq_train)[~np.isnan(RedChiSq_train)], \n",
    "         bins=10**np.arange(-1, 3, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "plt.xlabel(r'$\\chi_{{\\rm red}}^2$', fontsize=15)\n",
    "plt.ylabel('Number of sources', fontsize=15)\n",
    "plt.title('Training dataset', fontsize=15)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.array(RedChiSq_val)[~np.isnan(RedChiSq_val)], \n",
    "         bins=10**np.arange(-1, 3, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f21c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "plt.xlabel(r'$\\chi_{{\\rm red}}^2$', fontsize=15)\n",
    "plt.ylabel('Number of sources', fontsize=15)\n",
    "plt.title('Validation dataset', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('./lcs_image/ZTF_alerts_chisquare_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ba8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_train =TSNE(n_components=2, perplexity=100, random_state=32, n_iter=1000).fit_transform(encoding_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
